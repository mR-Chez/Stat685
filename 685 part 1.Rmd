---
title: "685 Part 1"
author: "Matt Chesney"
date: "6/5/2022"
output: html_document
---

```{r , echo=F, eval=F}
#data is built into package, this is an example for reference 
library(lme4)

# NOT RUN {
str(sleepstudy)
require(lattice)
xyplot(Reaction ~ Days | Subject, sleepstudy, type = c("g","p","r"),
       index = function(x,y) coef(lm(y ~ x))[1],
       xlab = "Days of sleep deprivation",
       ylab = "Average reaction time (ms)", aspect = "xy")
(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy,REML=F))
## independent model
(fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy, REML=F))
# }
(fm3 <- lmer(Reaction ~ Days + (Days||Subject), sleepstudy, REML=F))
```

## Introduction 

This is a tutorial to walk through of how to complete Mixed regression in R. The tutorial is in two parts, the first is mixed regression utilizing linear methods and part 2 is generalized linear regression. 


## Mixed models 
What are mixed models? introduce fixed and random effects and how they differ.

In an experiment there is a unit that the experiment is being conducted on, this is the **experimental unit**. For the Experimental unit there is some measured attribute of interest, this is the **independent variable** or our Y.  

A **fixed effect** is a treatment that is applied to our experimental unit. The treatment's effect is ascertained by a shift in the average value of Y, or the independent variable being monitored.

The NULL hypothesis is that the treatment has no effect on the mean. INSERT equations for null hypothesis and test statistic here. 

Examples:   
1. Comparing tensile strengths of alloy blends.  
2. Comparing weight gain in pigs from several food types  
3. Comparing maximum speeds of cars with different fuel additives
4. Comparing the blood pressure change in a patient over several days of medication 

A **random effect** is also applied to the experimental unit. The treatment's effect is determined by a difference in the variation in Y due to the application of the treatment. A random effect is assigned when the population of treatments is assumed to be homogeneous. 

The NULL hypothesis is that the treatment has no effect on the variance. INSERT equations for null hypothesis and test statistic here. 

Examples:  
1. A factory's various production lines effect on variation tensile strength.  
2. A pig farm's pig pens' effect on variation in weight gain  
3. A professional driver's effect on variation in top speeds of cars. 
4. A medical subject's effect on blood pressure medication treatment. 

The mixed model contains both fixed effects and random effects.There are several ways this relationship can manifest. The fixed effect can have a consistent effect within the random effect, intercept changes but slope remains the same. The fixed effect random effect can only vary in the presence of the fixed effect, intercept constant and slope varies. The fixed effect can vary with respect to the random effect, slope and intercept change. Each of these options will be examined in our sleep data example. 

Examples:  
1. Comparing tensile strengths of alloy blends from several production lines in a factory.  
2. Comparing weight gain in pigs from several food types living in one of several randomly assigned pens  
3. Comparing maximum speeds of cars with different fuel additives driven by randomly assigned professional drivers. 
4. Comparing the average blood pressure reduction after a treatment over several days applied to randomly selected subjects. 


## Introduction to the Data
The sleepstudy dataset is available as part of the lme4 package. It is from a study conducted by Belenkyn et all (2003). 

The response variable is the reaction time of the subjects to stimuli as recorded by researchers `Reaction`. The Subjects are the random effect, Subject. The number of days the subject underwent sleep deprivation is the fixed effect `Days`. 

```{r, echo=F}
library(lme4)
library(dplyr)
library(lattice)
library(ggplot2)
 
```
Below is the structure of the Sleep Study data 
```{r}
str(sleepstudy)
```


The first few lines of data. These are all observations for subject 308. 
```{r}

head(sleepstudy)
```

Plot of Sleep Study data, color coded by patient numbers  
```{r}


ggplot(sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject)) + 
    geom_point(size=1) + theme_classic()

```



## The Model
This example will follow the convention of using Greek symbols to denote fixed effects and capital letters to denote random effects. 

The variables in out model are: $y$ is the reaction time of our subjects, $\beta$ is the Days coefficient  
$C$, our random effect will be split into two parts: $C_{intercept}$ and $C_{slope}$. Our model for this example is below. 

Model 1, Only intercept varies 

$$y_{ij} = \alpha_{intercept}  + c_{i} + \beta_{j}*Days + e_{ij}$$



Model 2, intercept and slope vary and random terms are correlated  

$$y_{ij} = \alpha_{intercept}  + c_{i} + (\beta_{j}+d_{i})*Days + e_{ij}$$
Model 3, intercept and slope vary and random terms are not correlated  

$$y_{Reaction} = \alpha_{intercept}  + C_{intercept} + \beta*Days + \beta C_{slope}*Days + e_{residual}$$


## LME4 Package
To analyze the mixed effect model we will utilize the R package LME4. 

The relevant syntax for the model is as follows: 

Random intercept effect: `(1|RandomEffect)`

Random Slope effect: `(0+FixedEffect|RandomEffect)`

Random intercept and slope with random effect correlated:  
`1+FixedEffect+(1+FixedEffect|RandomEffect)`  
Abbreviation:  
`FixedEffect+(FixedEffect|RandomEffect`

Random intercept and slope with random effect uncorrelated:  
`1+FixedEffect+(1|RandomEffect)+(0+FixedEffect|RandomEffect)`  
Abbreviation:  
`FixedEffect+(FixedEffect||RandomEffect)`

We will utilize the abbreviated syntax where appropriate. 

### REML

Explain why REML is better to use than log likelihood 

## Model 1 

$$y_{Reaction} = \alpha_{intercept}  + C_{intercept} + \beta*Days + e_{residual}$$
```{r}
#add object to hold AIC, BIC, and LOG lik for four models 
Ac=0;Bc=0;ll=0 
```

For model comparison we will use the option ` REML=F`, once the model is selected we will use REML 

```{r, echo=F}
model1 <- lmer(Reaction ~ Days + (1|Subject), sleepstudy,REML=F)

summary(model1)
i=1
Ac[i]=AIC(model1)
Bc[i]=BIC(model1)
ll[i]=logLik(model1)

ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model1)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```

## Model 2 


$$y_{Reaction} = \alpha_{intercept} + \beta*Days + \beta C_{slope} + e_{residual}$$
```{r}
model2 <- lmer(Reaction ~ Days + (0+ Days|Subject), sleepstudy,REML=F)
summary(model2)
i=2
Ac[i]=AIC(model2)
Bc[i]=BIC(model2)
ll[i]=logLik(model2)

ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model2)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```

## Model 3 
$$y_{Reaction} = \alpha_{intercept}  + C_{intercept} + \beta*Days + \beta C_{slope}*Days + e_{residual}$$
### Correlated
```{r}
model3c <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy,REML=F)
summary(model3c)
i=3
Ac[i]=AIC(model3c)
Bc[i]=BIC(model3c)
ll[i]=logLik(model3c)

ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model3c)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```

### Uncorrelated
```{r}
model3u <- lmer(Reaction ~ Days + (Days||Subject), sleepstudy,REML=F)
summary(model3u)
i=4
Ac[i]=AIC(model3u)
Bc[i]=BIC(model3u)
ll[i]=logLik(model3u)


ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model3u)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()


```

```{r,echo=F}
#plot AIC, BIC, log lik of 4 models, explain which one will be used, 
name=c('model 1','model 2', 'model 3c','model 3u')
names(Ac)=name
names(Bc)=name
names(ll)=name

plot(Ac, ylab='AIC',type='h',lwd=20,xaxt='n'); axis(1,at=1:4,names(Ac))

plot(Bc, ylab='BIC',type='h',lwd=20,xaxt='n'); axis(1,at=1:4,names(Bc))

plot(ll, ylab='Log lik',type='h',lwd=20,xaxt='n'); axis(1,at=1:4,names(ll))

anova(model1,model2,model3c,model3u)



```
AIC and BIC select the uncorrelated model, log likelihood selects the correlated model. We will   

## Output 

## Explantion of output 

## bootstrap confidence intervals 

# Bayseian 
Bayesian methods are another option to approach mixed effects problems. Bayesian methods are especially helpful with smaller sample sizes, but are great for large samples as well. We will be using the *brms* package, which uses syntax similar to the *lmer* package. 

## Model 1b 
```{r}
require(rstan)
require(brms)

priors <- c(prior(normal(298.5079, 1), class = Intercept), # intercept prior
            prior(normal(10.47, 1), class = b), # slope prior
            prior(normal(0, 3172), class = sigma), # population variance
            prior(normal(0, 56), class = sd) # tau0, group variance
            )

model1b <- brm(Reaction ~ Days + (1 | Subject),
             data = sleepstudy,
             prior = priors,
             family = gaussian()
             )
model1b <- add_criterion(model1b, "waic")

summary(model1b)
```
As you can see, we get similar estimates for slope and intercept using this method that we did using the lmer package. 

##INSERT## Package information, explanation of output.

We can also get posterior estimates along with 95% credible intervals. The intercept estimates are shown as values in relation to the population-level effects (b_Intercept).

##INSERT## explain the difference between credible interval and confidence interval.

```{r}
posterior_summary(model1b)    
```

## Model 2b
```{r}
model2b <- brm(Reaction ~ Days + (0 + Days | Subject),
             data = sleepstudy,
             prior = priors,
             family = gaussian()
             )
model2b <- add_criterion(model2b, "waic")

summary(model2b)
```
We can look at the estimated slopes from the posterior summary function call.

```{r}
posterior_summary(model2b)
```

## Model 3bc - Correlated 
```{r}
model3bc <- brm(Reaction ~ Days + (Days | Subject),
             data = sleepstudy,
             prior = priors,
             family = gaussian()
             )
model3bc <- add_criterion(model3bc, "waic")

summary(model3bc)
posterior_summary(model3bc)
```
Now we can see all estimates for slopes and intercepts in each group and population-wide.

```{r}
posterior_summary(model3bc)
```

## Model 3bu - Uncorrelated
```{r}
model3bu <- brm(Reaction ~ Days + (Days || Subject),
             data = sleepstudy,
             prior = priors,
             family = gaussian()
             )
model3bu <- add_criterion(model3bu, "waic")

summary(model3bu)
```
```{r}
posterior_summary(model3bu)
```

## Model Comparison

How do we compare these models against each other? brms has two options - LOO (leave one out cross validation) and WAIC (widely applicable information criterion) INSERT - explain the pros and cons 

```{r}
waic_1b <- waic(model1b)$estimates[3,1]
waic_2b <- waic(model2b)$estimates[3,1]
waic_3bc <- waic(model3bc)$estimates[3,1]
waic_3bu <- waic(model3bu)$estimates[3,1]

waic_df <- data.frame(model = c("1b", "2b", "3bc", "3bu"),
                      waic = c(waic_1b, waic_2b, waic_3bc, waic_3bu))

waic_df

```

## The Bayseian model 

## Output 

## Explanation of output 

## Bayseian Bootstrap confidence intervals 

# Sources 

https://cran.microsoft.com/snapshot/2021-10-10/web/packages/lme4/vignettes/lmer.pdf  
https://www.lcampanelli.org/mixed-effects-modeling-lme4/  
https://www.rensvandeschoot.com/tutorials/lme4/  
https://vitalflux.com/fixed-vs-random-vs-mixed-effects-models-examples/  
https://www.rdocumentation.org/packages/lme4/versions/1.1-29/topics/lmer  
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5970551/  
https://towardsdatascience.com/a-bayesian-approach-to-linear-mixed-models-lmm-in-r-python-b2f1378c3ac8
https://towardsdatascience.com/evaluating-bayesian-mixed-models-in-r-python-27d344a03016
https://cran.r-project.org/web/packages/brms/index.html
https://yjunechoe.github.io/posts/2020-06-07-correlation-parameter-mem/

Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003). 
