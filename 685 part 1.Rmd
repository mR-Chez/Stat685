title: "685 Part 1"
author: "Matt Chesney & Casey Moroney"
date: "6/5/2022"
output: html_document
---

```{r , echo=F, eval=F}
#data is built into package, this is an example for reference 
library(lme4)
# NOT RUN {
str(sleepstudy)
require(lattice)
xyplot(Reaction ~ Days | Subject, sleepstudy, type = c("g","p","r"),
       index = function(x,y) coef(lm(y ~ x))[1],
       xlab = "Days of sleep deprivation",
       ylab = "Average reaction time (ms)", aspect = "xy")
(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy,REML=F))
## independent model
(fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy, REML=F))
# }
(fm3 <- lmer(Reaction ~ Days + (Days||Subject), sleepstudy, REML=F))
```

## Introduction 

This is a tutorial to walk through of how to complete Mixed regression in R. The tutorial is in two parts, the first is mixed regression utilizing linear methods and part 2 is generalized linear regression. 


## Mixed models 
What are mixed models? introduce fixed and random effects and how they differ.

In an experiment there is a unit that the experiment is being conducted on, this is the **experimental unit**. For the Experimental unit there is some measured attribute of interest, this is the **independent variable** or our Y.  

A **fixed effect** is a treatment that is applied to our experimental unit. The treatment's effect is ascertained by a shift in the average value of Y, or the independent variable being monitored.

The NULL hypothesis is that the treatment has no effect on the mean. $\beta$ is our Fixed effect 


$$H_0: \beta=0$$
$$H_a: \beta \neq 0$$


Examples:   
1. Comparing tensile strengths of alloy blends.  
2. Comparing weight gain in pigs from several food types  
3. Comparing maximum speeds of cars with different fuel additives  
4. Comparing the blood pressure change in a patient over several days of medication 

A **random effect** is also applied to the experimental unit. The treatment's effect is determined by a difference in the variation in Y due to the application of the treatment. A random effect is assigned when the population of treatments is assumed to be homogeneous. 

The NULL hypothesis is that the treatment has no effect on the variance. $\sigma_{R}$ is the variance attributed to our treatment.


$$H_0: \sigma_{R}=0$$
$$H_a: \sigma_{R}\neq 0$$
Examples:  
1. A factory's various production lines effect on variation tensile strength.  
2. A pig farm's pig pens' effect on variation in weight gain  
3. A professional driver's effect on variation in top speeds of cars.  
4. A medical subject's effect on blood pressure medication treatment. 

The mixed model contains both fixed effects and random effects.There are several ways this relationship can manifest. The fixed effect can have a consistent effect within the random effect, intercept changes but slope remains the same. The fixed effect random effect can only vary in the presence of the fixed effect, intercept constant and slope varies. The fixed effect can vary with respect to the random effect, slope and intercept change. Each of these options will be examined in our sleep data example. 

Examples:  
1. Comparing tensile strengths of alloy blends from several production lines in a factory.  
2. Comparing weight gain in pigs from several food types living in one of several randomly assigned pens  
3. Comparing maximum speeds of cars with different fuel additives driven by randomly assigned professional drivers.  
4. Comparing the average blood pressure reduction after a treatment over several days applied to randomly selected subjects. 


## Introduction to the Data
The sleepstudy dataset is available as part of the lme4 package. It is from a study conducted by Belenkyn et all (2003). 

The response variable is the reaction time of the subjects to stimuli as recorded by researchers `Reaction`. The Subjects are the random effect, Subject. The number of days the subject underwent sleep deprivation is the fixed effect `Days`. 

```{r, echo=F, message=F}
library(lme4)
library(dplyr)
library(lattice)
library(ggplot2)
 
```
Below is the structure of the Sleep Study data 
```{r}
str(sleepstudy)
```


The first few lines of data. These are all observations for subject 308. 
```{r}
head(sleepstudy)
```

Plot of Sleep Study data, color coded by patient numbers  
```{r}
ggplot(sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject)) + 
    geom_point(size=1) + theme_classic()
```



## The Model
This example will follow the convention of using Greek symbols to denote fixed effects and capital letters to denote random effects. 

The variables in out model are: $y$ is the reaction time of our subjects, $\beta_j$ is the Days coefficient  
$c_i$, our random intercept term. 

Model 1, Only intercept varies [Fig 1]

$$y_{ij} = \alpha_{intercept}  + c_{i} + \beta_{j}*Days + e_{ij}$$


Model 2, intercept and slope vary and random terms are correlated:  $\rho_{c_j,d_i} \ne 0$  [Fig 2]
Here $d_i$ is our random slope term. 


$$y_{ij} = \alpha_{intercept}  + c_{i} + (\beta_{j}+d_{i})*Days + e_{ij}$$
Model 3, intercept and slope vary and random terms are not correlated: $\rho_{c_j,d_i} = 0$ [Fig 3]


$$y_{ij} = \alpha_{intercept}  + c_{i} + (\beta_{j}+d_{i})*Days + e_{ij}$$


## LME4 Package
To analyze the mixed effect model we will utilize the R package LME4. The relevant syntax for the model is as follows:

|Model Type|Syntax|Abbreviated Syntax|
|--------------------------------------------------------|------------------------------------|-----------------------------|------------------------|
|Random intercept |`(1|RandomEffect)`||
|Random Slope effect| `(0+FixedEffect|RandomEffect)`||
|Random intercept and slope with random effect correlated|`1+FixedEffect+(1+FixedEffect|RandomEffect)`|`FixedEffect+(FixedEffect|RandomEffect`|
|Random intercept and slope with random effect uncorrelated|`1+FixedEffect+(1|RandomEffect)+(0+FixedEffect|RandomEffect)` |`FixedEffect+(FixedEffect||RandomEffect)`|


We will utilize the abbreviated syntax where appropriate.  

### REML
Restricted Maximum likelihood is recommended for the final model, however REML does not work with model selection techniques such as AIC and BIC. If it is necessary to compare models REML should not be used. Once the best model is selected REML can be used with that model for more accurate confidence intervals.This can be achieved in the LME4 package by adding  `REML=False` as an option. The default in LME4 is true.  

```{r, echo=F}
#add object to hold AIC, BIC, and LOG lik for four models 
Ac=0;Bc=0;ll=0 
```

For model comparison we will use the option ` REML=F`, once the model is selected we will use REML 

### Model 1 (Fig 1)



```{r}
model1 <- lmer(Reaction ~ Days + (1|Subject), sleepstudy,REML=F)
summary(model1)
```
#### Output  
The summary of the model gives us AIC, BIC, and log likelihood scores along with deviance and degrees of freedom. After the scaled residuals we get parameter estimates. listed first are estimates for the random effects. The $c_i$ standard deviation is estimated at 36.01 and the $e_{ij}$ is estimated at 30.9. Next are the estimates for the fixed effects of intercept and the `Days` term. 


### Visulization of model 1 

The lines in this graph are the model for each of the `Subjects`. 
$$y_{ij} = \alpha_{intercept}  + c_{i} + \beta_{j}*Days $$


```{r}
ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model1)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```



```{r, echo=F}
i=1
Ac[i]=AIC(model1)
Bc[i]=BIC(model1)
ll[i]=logLik(model1)
```


## Model 2 [Fig 2]

```{r}
model2 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy,REML=F)
summary(model2)
```
### Output  
In addition to the output received in model 1, we also get an estimate for the $d_i$ term in the random effects listed in the `Days` row. We also get the $\rho$ estimate for $(c_i , d_i)$ which is .08. 

### Visualization of model 2  
The lines on this plot represent each of the subjects with the correlation term following the equation: 
$$y_{ij} = \alpha_{intercept}  + c_{i} + (\beta_{j}+d_{i})*Days$$



```{r}
ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model2)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```


```{r, echo=F}
i=2
Ac[i]=AIC(model2)
Bc[i]=BIC(model2)
ll[i]=logLik(model2)
```

### Model 3 [Fig 3]
```{r}
model3 <- lmer(Reaction ~ Days + (Days||Subject), sleepstudy,REML=F)
summary(model3)
```
### Output  
The output for model 3 is in the same format as model 2 but without the correlation term. 

### Data visulization for model 3
Each line represents the subject without a random effects correlation term according to the equation: 
$$y_{ij} = \alpha_{intercept}  + c_{i} + (\beta_{j}+d_{i})*Days$$


```{r}
ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model3)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```


```{r, echo=F}
i=3
Ac[i]=AIC(model3)
Bc[i]=BIC(model3)
ll[i]=logLik(model3)
```

Here we compare the AIC, BIC, and log likelihood scores for each of the models. 

```{r,echo=F}
name=c('model 1','model 2', 'model 3')
names(Ac)=name
names(Bc)=name
names(ll)=name
plot(Ac, ylab='AIC',type='h',lwd=20,xaxt='n'); axis(1,at=1:3,names(Ac))
plot(Bc, ylab='BIC',type='h',lwd=20,xaxt='n'); axis(1,at=1:3,names(Bc))
plot(ll, ylab='Log lik',type='h',lwd=20,xaxt='n'); axis(1,at=1:3,names(ll))
anova(model1,model2,model3)
```
AIC and BIC select the uncorrelated model ( model 3), log likelihood selects the correlated model (model 2) . We will go with model 3.  

## Confidence intervals for mixed parameters 

### Refit with REML

First, as mentioned previously we will refit the selected model with `REML=T`. 

```{r}
model3 <- lmer(Reaction ~ Days + (Days||Subject), sleepstudy,REML=T)
```

### Confidence intervals in LME4  
The lme4 package contains a `confint.mermod()` function that computes confidence intervals for fixed and random effects. The function is shown below with all of its parameters. 

```{r,eval=F}
# S3 method for merMod credit: https://www.rdocumentation.org/packages/lme4/versions/1.1-29/topics/confint.merMod
confint(object, parm, level = 0.95,
	method = c("profile", "Wald", "boot"), zeta,
	nsim = 500,
        boot.type = c("perc","basic","norm"),
        FUN = NULL, quiet = FALSE,
	oldNames = TRUE, ...)
```

By default all parameters will be output at a 95% interval utilizing the profile likelihood method. Below we show the confidence intervals for our selected model. The parameters are output in the same order as model summary. sig01 is the $c_i$ term, sig02 is the $d_j$ term, sigma is the $e_{ij}$ term. fixed effect terms are listed by name. 

```{r}
confint.merMod(model3)
```

### Bootstrap Confidence intervals 

Next we use the same function to calculate the bootstrap confidence intervals for all parameters. The default number of iterations is 500. 

```{r}
confint.merMod(model3, method = 'boot')
```


### Extracting Design matrix 

The design matrix for model 3: 
$$ y= X \beta+Zu+e$$ 

Eache element can be extracted as seen below: 
```{r}
Xi=getME(model3,"X")
Zi=getME(model3,"Z")
Ui=getME(model3,"u")
beta=getME(model3,"beta")
```

***

# Bayesian Methods
Bayesian methods are another option to approach mixed effects problems. Bayesian methods are especially helpful with smaller sample sizes, but are great for large samples as well. We will be using the *brms* package, which uses syntax similar to the *lme4* package. 

### Introduction to the dataset
We will be using a dataset that comes with the *lme4* pacakge called "sleepstudy". This dataset comes from a study conducted by Belenkyn et al (2003). 

In this dataset, we have 18 subjects with varying degrees of sleep deprivation.

```{r}
str(lme4::sleepstudy)
```

The response variable, `Reaction`, represents the average reaction time in milliseconds observed for subjects in a sleep deprivation study. `Days` represents the number of days the given `Subject` was deprived of sleep.

```{r}
head(lme4::sleepstudy)
```
Below we have a simple plot of the dataset.

```{r}
lme4::sleepstudy %>% 
    ggplot(aes(x = Days, y = Reaction)) + geom_point() + theme_classic()
```

Using the parameter `color = Subject` within the `aes()` call, we can color each point by Subject. For example, the top of the legend shows that Subject 309 is depicted by deep orange dots:

```{r}
lme4::sleepstudy %>% 
    ggplot(aes(x = Days, y = Reaction, color = Subject)) + geom_point() + theme_classic()
```


### Bayes Theorem
In Bayesian statistics the modelling approach is based off of Bayes' theorem, where we use information in the observed data to update knowledge about the model parameters. The background information is known as the prior distribution and combined with likelihood functions to determine the posterior distribution.

$$P(\theta, y)=P(\theta)P(y|\theta)$$
Where the posterior distribution is represented by: 
$$P(\theta, y)$$

The prior distribution:
$$P(\theta)$$
and the sampling distribution represented by:
$$P(y|\theta)$$
Given this information, we can use our data (sampling distribution) along with a prior to calculate the posterior.

***
## Model 1  
Our first model assumes that the slope (reaction time over number of days) is the same for each subject. In this case, the goal is to estimate the intercept, which represents the baseline reaction time without sleep deprivation, for each subject. 

$$y_{ij} = \beta_0 + \beta_{1}t_j + C_{i} + e_{ij}$$
where $y_{ij}$ is the reaction time of the $i^{th}$ subject at j days, $\beta_0$ is the population intercept, $\beta_1$ is the population slope, $t_j$ is the number of days at time $t$, $C_i$ is the random intercept for subject $i$, and $e_{ij}$ is the error term. Since the time markers are in days that are equally spread apart, ${t_j}$ is simply ${j}$, number of days.

Assumptions for this model:   
1. $e_{ij}$'s are independent with mean equal to 0, finite variance, and follow $Normal(0,\sigma^2_e )$  
2. $C_1, C_2, ..., C_n$ are all independent and follow $Normal(0,\sigma^2_c)$   
3. $e_{ij}$ and $C_i$ are independent    
4. $e_{ij}$, $C_i$, and $t_j$ are all independent   

where,  
$$i \in \{1,...,18\} , and$$  
$$j \in \{0,...,9\}$$   


### Priors
We need to choose prior distributions for parameters that might be helpful in forming a posterior distribution. I will use fitted parameters from a mixed linear model on the same data built using the `lme4` package. See the `lme4` section of this manual for more info.

First, load in the *rstan* and *brms* packages, then build our base model.

```{r, message=FALSE, results}
require(rstan)
require(brms)

# Fit a base model using lmer
lmm <- lmer(Reaction ~ Days + (1|Subject), sleepstudy)

# Pull base model estimates from lmer model
b0 <- summary(lmm)$coefficients[1,1]
b1 <- summary(lmm)$coefficients[1,2]
se <- summary(lmm)$sigma
te <- attr(summary(lmm)$varcor$Subject, "stddev")
```

We are using the base model slope and intercept estimates as prior distributions of $\beta_1$ and $\beta_0$, the standard deviation of residuals for $\sigma_e$, and the standard deviation of subject specific random intercept $\tau_o$.

The priors we are using:
$$\beta_0 \sim N(251, 10)$$
$$\beta_1 \sim N(10, 10)$$
$$\sigma_e \sim N(0, 31)$$
$$\tau_0 \sim N(0, 37)$$

## BRMS package - Sampler
As mentioned before, this package does not fit models itself but uses `Stan` on the back end. Stan is a probabilistic programming language with a C++ backend. By default this backend uses the static Hamiltonian Monte-Carlo (HMC) Sampler and its extension the No-U-Turn Sampler (NUTS) by Hoffman and Gelman(2014). 

Alternatives to these algorithms, not available in the `brms` package include random-walk Metropolis and variations, Gibbs-sampling, and slice-sampling. The documentation for this package concludes that its choice of samplers converge much more quickly especially for high-dimensional models regardless of whether the priors are conjugate or not. Read the `brms` documentation overview for more information.

### The brm() function
This function in the `brms` package fits generalized linear/non-linear multivariate multilevel models (MMM). To set priors, we use the `prior()` function, which accepts a distribution with parameters as well as a `class` argument which designates which parameter the prior is associated with. The `brm` function does not handle parameters set as variables inside of `prior()` well, so you will need to manually enter the prior values into the `prior()` function. 

### Default arguments for brm() function call
**Chains**: The number of Markov chains to use. The advantage of using more chains is that we can be more confident of our understanding of the posterior distribution as each chain has a different starting point. If running in parallel using the "cluster" option, this package limits the number of chains to the number of cores the system has. Defaults to 4.

**Iter**: The total number of iterations per chain. Some datasets may require more iterations to converge - meaning it takes longer to reach a point where further iterations do not improve the model. A higher number will cause the model to take longer to fit, see a comparison below. Defaults to 2,000.

**Warmup**: Also known as burn-in. This refers to the number of iterations at the beginning of an MCMC run to be thrown away. The warm-up period is a hyper-parameter that is meant to give the Markov Chain time to reach its equilibrium distribution. The idea here is that a "bad" randomized starting point may over-sample regions that aren't representative of the equilibrium distribution. Defaults to `floor(iter/2)`.

**Thin**: Thinning consists of picking values at each k-th step and removing them from the Markov chain. This helps speed up model convergence if number of iterations, `iter`, is large. Defaults to 1, meaning all values are kept. Choose a value > 1 to save memory and computation time if `iter` is large.

**Cores**: Number of CPU cores to utilize for building chans. As chains can be created independent of one another, parallelization can be utilized to speed up the process. You can see how many cores are available on your machine by using the `detectCores()` function from the package `parallel`.

**Family**: Defaults to Gaussian. Since we are building a *linear* mixed model, we will use leave use the default. More info on specifying distribution family can be found in the generalized mixed model section of this manual.

```{r, message=FALSE, warning=FALSE, echo=TRUE, cache=TRUE, results='hide'}
priors <- c(prior(normal(251, 10), class = Intercept), # intercept prior
            prior(normal(10, 10), class = b), # slope prior
            prior(normal(0, 31), class = sigma), # population variance
            prior(normal(0, 37), class = sd) # tau0, group variance
            )

model1 <- brm(Reaction ~ Days + (1 | Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 10000,
             # warmup = ,
             thin = 1, 
             cores = 4,
             family = gaussian()
             )

summary(model1)
```

**Notes**: Output text is truncated. To compare these results with the output from a model built with the `lme4` package, please see the Frequentist section of this manual above. 

```{r, iteration_comparison, eval=FALSE, echo=FALSE}

time_start <- Sys.time()
model1_2k <- brm(Reaction ~ Days + (1 | Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 2000,
             thin = 1, 
             cores = 4,
             family = gaussian()
             )
time_end <- Sys.time()
time_2k_iter <- time_end - time_start

time_start <- Sys.time()
model1 <- brm(Reaction ~ Days + (1 | Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 10000,
             thin = 1, 
             cores = 4,
             family = gaussian()
             )

time_end <- Sys.time()
time_10k_iter <- time_end - time_start

time_2k_iter
time_10k_iter
```

More iterations take longer to fit. If a model fails to converge, you will see a warning output and may need to adjust iterations or other fitting options in order to be successful. Experiment with iterations until you reach an acceptable output. 

If you want to compare fitting times for different iterations, un-comment the code block above this section in the markdown file for an example comparison.

***

## Model Summary
With our model built, we can now get estimates of the parameters. R's built-in `summary()` function does a good job explaining each portion of the output one item at a time.

```{r}
summary(model1)
```

The first 5 lines of the summary output describe our model and model inputs defined in the `brm` call. 

### Group-Level Effects
If you want to access a specific estimate from the model object, they can be accessed by using the `$` operator on the summary object of the model. For example: `summary(model1)$random$Subject` returns Subject-specific (or group level) random effects:

```{r, echo=FALSE, message = FALSE}
summary(model1)$random$Subject
```

If you want to access individual parts of this output, the `$` notation can be used in addition to the last command to access each part.

Our estimate for the subject-specific random effect is `r round(summary(model1)$random$Subject$Estimate,2)`, which differs from the base model estimate $\tau_0$ =  `r round(te)` although the base model estimate is still within the Bayesian credible interval **(`r summary(model1)$random$Subject$"l-95%"`, `r summary(model1)$random$Subject$"u-95%"`)**.

<!-- This out put shows Est.Error of `r round(summary(model1)$random$Subject$Est.Error,2)` which represents the standard deviation of the distribution of the posterior.  -->

**Rhat** ($\hat R$) for a given parameter estimate is a measure of whether the samples obtained are likely representative of the true distribution. Put simply, it is the square root of the estimated total variance over within-chain variance. 
It's formula: 

$$\hat R=\sqrt{\hat {var}^+ / W}$$

where,

* Total variance estimate ${\hat {var}}^+$ = $({n-1} / n)*W + (1/n)*B$
* The within-chain variance estimate $W=(1/m)\sum_{j=1}^m s_j^2$, where $s^2_j=(1/n-1)\sum_{i=1}^n (\psi_{ij}-\bar \psi_{.j})$
* Between-chain variance estimate $B=(n/m-1)\sum_{j=1}^m(\bar\psi_{.j}-\bar\psi_{..})^2$, where $\bar\psi_{.j}=(1/n)\sum_{i=1}^n\psi_{ij}$, $\bar\psi_{..}=(1/m)\sum_{j=1}^m\bar\psi_{.j}$

The STAN backend code uses split $\hat R$ that takes N chains, splits them in half, then performs the $\hat R$ calculation on the resulting $2N$ chains. So if there is only one chain, $\hat R$ can still be calculated.

If this value is larger than 1.1, it is an indication that the model has not converged adequately. This can be accessed for each parameter estimate within the model summary. For example, $\hat R$ for subject specific random intercept, $\hat R_{\hat\sigma_\tau}$ is accessed with `summary(model1)$random$Subject$Rhat` = `r summary(model1)$random$Subject$Rhat`. 

**Bulk_ESS** or bulk effective sample size is a diagnostic for the sampling efficiency in the bulk of the posterior distribution. It measures the information content of a sample chain. It can be thought of as the minimum sample size from the posterior which has the same efficiency in estimating the posterior as a chain of MCMC samples. A higher number here is better. ESS is also used to determine if the $\hat R$ is reliable - if ESS is under 400 we cannot be certain that it is reliable.

<!-- $$ESS = NM / (1+2\sum_{t=1}^{T}\hat\rho_t)$$ -->
$$ESS = NM /\hat\pi$$
where $N$ is the length of the chain, $M$ is the total number of chains, and $\hat\pi$ represents estimated correlation between lagged samples in the chain.

**`Tail_ESS`** represents the minimum sample size needed to create 5% and 95% quantiles of the posterior. More information can be found in Vehtari, et al (2021).

### Population-Level Effects
```{r, echo=FALSE, results='markup'}
pop_effects <- summary(model1)$fixed
```

This section shows our intercept estimate, $\hat\beta_0$ = `r round(pop_effects$Estimate[1],2)`, our slope estimate, $\hat\beta$ = `r round(pop_effects$Estimate[2],2)` along with credible intervals, $\hat R$, and bulk/tail ESS for each.

### Family Specific Parameters:
```{r, echo=FALSE}
fam_effects <- summary(model1)$spec_pars
```
This shows our estimate for $\hat\sigma_e$ = `r round(fam_effects$Estimate,2)`, compared to our estimate from the `lme4` model `r round(se,2)`.


### Posterior Summary
While the `summary()` function has a tidy appearance, the `posterior_summary()` function produces much more information about the estimates. 
```{r}
posterior_summary(model1)
```

The first 4 lines are repeated from the summary, along with estimate quantiles. If you want to show trace and density plots for each model parameter, use `plot(model)`:
```{r}
plot(model1)
```

The left side of the plot shows the overall density of our parameter estimates. The right side of the plot output shows a trace plot with parameter estimates across all iterations. 

The next section of our model output displays $C_{i}$, our intercept estimates for each subject. The means are interpreted as where the intercept is on the Y-axis in relation to the population mean. For example - subject 308 has an intercept mean of 88.46, so this could be interpreted as the subject having a reaction time of (88.46 + 204.98) = 293.44ms. 

`lprior` and `lp__` are used for automatic prior/likelihood sensitivity analysis using an additional package, `priorsens` and are not included in the `brms` documentation.

## Plotting Model Estimates
As an alternative the using the `summary(model)$` notation, it's also possible to pull model coefficients and error estimates using the `coef()` function. We can access intercept estimates for each Subject, $\hat C_i$, to plot them along with the dataset.

If you would like to see example code on how to do this, view hidden code in this section.

```{r, echo=FALSE}
# population level slop estimate
pop_slope <- coef(model1)[["Subject"]][,,'Days'][1,1]
m1_coefs <- coef(model1)[["Subject"]][,,'Intercept'][,1]

estimate_df <- data.frame(subject = factor(names(m1_coefs)),
           slope = pop_slope,
           intercept = m1_coefs)


# Plotting first 4 subjects
# Function to emulate first 4 default colors from ggplot package
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors = gg_color_hue(4)

sleepstudy %>% 
    filter(Subject %in% c(308,309,310,330)) %>% # first 4 subjects
    ggplot(aes(x = Days, y = Reaction, color = Subject)) +
    geom_point() +
    # subject 308
    geom_abline(slope = estimate_df[1,]$slope, 
                intercept = estimate_df[1,]$intercept,
                color = colors[1]) +
    # subject 309
    geom_abline(slope = estimate_df[2,]$slope,
                intercept = estimate_df[2,]$intercept,
                color = colors[2]) +
    # subject 310
    geom_abline(slope = estimate_df[3,]$slope,
                intercept = estimate_df[3,]$intercept,
                color = colors[3]) +
    # subject 330
    geom_abline(slope = estimate_df[4,]$slope,
                intercept = estimate_df[4,]$intercept,
                color = colors[4])
```

***

## Model 2
Our second model has a random intercept term and a random slope, with correlated random effect. 
$$y_{ij} = \beta_0  + \beta_1 t_j + C_{i,0}  + C_{i,1}*t_j + e_{ij}$$
Reaction time $y_{ij}$ is equal to population intercept $\beta_0$ plus population slope $\beta_1 t$ times number of days $t_j$, plus subject specific random intercept $C_{i,0}$ plus subject specific random slope $C_{i,1}$ times number of days $t_j$, plus an error term $e_{ij}$. $C_{i,0}$ and $C_{i,1}$ are both independent of $e_{ij}$

### Model 2 - Correlated 
We will first build this model allowing correlation between subject specific random effect and number of days. We will fit a base model using *lmer* with correlation denoted by `(Days | Subject)`. We will use the estimates from the base model as priors for a *brm* model. 

```{r, message=FALSE, warning=FALSE, echo=FALSE, cache=TRUE, results='hide'}
base_model2 <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)
summary(base_model2)

b0 <- summary(base_model2)$coefficients[1,1]
b1 <- summary(base_model2)$coefficients[2,1]
se <- summary(base_model2)$sigma
te <- attr(summary(base_model2)$varcor$Subject, "stddev")[1] # subject specific intercept 
be <- attr(summary(base_model2)$varcor$Subject, "stddev")[2] # random slope sd 

cor_int_slope <- attr(summary(base_model2)$varcor$Subject, "correlation")[1,2]

priors <- c(prior(normal(251, 10), class = Intercept), # intercept prior
            prior(normal(7, 10), class = b), # slope prior
            prior(normal(0, 26), class = sigma), # population variance
            prior(normal(0, 24), class = sd, coef=Intercept, group=Subject), # tau0, group intercept variance
            prior(normal(0, 6), class = sd, coef=Days, group=Subject) # group slope variance
            )

model2c <- brm(Reaction ~ Days + (Days | Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 10000,
             thin = 1, 
             cores = 4,
             family = gaussian()
             )
```
You can access model estimates, estimated error, 2.5% annd 97.5% credible intervals by using the `posterior_summary()` function on the model object.

```{r, echo=TRUE}
posterior_summary(model2c)
```

Now we can see all estimates for slopes and intercepts in each group and population-wide. The first two lines, `b_Intercept` and `b_Days` represent values for the estimates $\hat\beta_0$ and $\hat\beta_1$, respectively. The next two lines represent the standard deviation of the subject-level intercept, $\hat\sigma_{C,0}$ and slope, $\hat\sigma_{C,1}$

The next line, `cor_Subject__Intercept__Days` is the estimated correlation between subject specific random effect and days - which is `r round(posterior_summary(model2c)['cor_Subject__Intercept__Days', 'Estimate'], 2)` or a very weak to weak positive association. This suggests that subject specific random intercept is weakly positively correlated with the independent variable days. Sigma represents $\hat\sigma_e$.

The next 36 lines show our model estimates for intercept and days for each individual subject, along with posterior standard deviation (Est.Error), 2.5%, and 97.5% credible intervals.

```{r, eval=FALSE, echo = FALSE}
# coef(model2c)
# posterior_summary(model2c)
```
Data and model estimates can be accessed and plotted by first pulling the subject specific random slopes and intercepts using `coef()` and adding them to a plot with `abline()`. 

```{r, echo=TRUE, message=FALSE}
# population level slop estimate
pop_slope <- coef(model2c)[["Subject"]][,,'Days'][,1]
m2c_coefs <- coef(model2c)[["Subject"]][,,'Intercept'][,1]

estimate_df <- data.frame(subject = factor(names(m2c_coefs)),
           slope = pop_slope,
           intercept = m2c_coefs)


# Plotting first 4 subjects
# Function to emulate first 4 default colors from ggplot package
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors = gg_color_hue(4)

sleepstudy %>% 
    filter(Subject %in% c(308,309,310,330)) %>% # first 4 subjects
    ggplot(aes(x = Days, y = Reaction, color = Subject)) +
    geom_point() +
    # subject 308
    geom_abline(slope = estimate_df[1,]$slope, 
                intercept = estimate_df[1,]$intercept,
                color = colors[1]) +
    # subject 309
    geom_abline(slope = estimate_df[2,]$slope,
                intercept = estimate_df[2,]$intercept,
                color = colors[2]) +
    # subject 310
    geom_abline(slope = estimate_df[3,]$slope,
                intercept = estimate_df[3,]$intercept,
                color = colors[3]) +
    # subject 330
    geom_abline(slope = estimate_df[4,]$slope,
                intercept = estimate_df[4,]$intercept,
                color = colors[4]) +
  ggtitle("Sleep Study Model 2 - Correlated")
```

### Model 2 - Uncorrelated
Now we build the model with random effects `Days` and `Subject` uncorrelated. This assumes that the subject specific random effect is uncorrelated with days. This is denoted by `(Days || Subject)` in the lmer and brm calls. As before, we will build a base model using `lmer` for prior estimation. 

```{r, message=FALSE, echo=TRUE, cache=TRUE, results='hide'}
base_model3 <- lmer(Reaction ~ Days + (Days || Subject),data = sleepstudy)

b0 <- summary(base_model3)$coefficients[1,1]
b1 <- summary(base_model3)$coefficients[1,2]
se <- summary(base_model3)$sigma
te <- attr(summary(base_model3)$varcor$Subject, "stddev")[1] # subject specific intercept 
be <- attr(summary(base_model3)$varcor$Subject, "stddev")[2]


model2u <- brm(Reaction ~ Days + (Days || Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 10000,
             thin = 1, 
             cores = 4,
             family = gaussian()
             )

# posterior_summary(model2u)
```

```{r}
summary(model2u)
```

The output of the summary function is similar to that of the correlated model, with the exception of showing the correlation between the random intercept and days as we are not allowing for correlation with this model. You can use the `posterior_summary()` function to see estimates and credible intervals for each parameter.

```{r, eval=FALSE, echo=FALSE}
posterior_summary(model2u)
```

We can also pull estimates from the model object to plot alongside our data as we did before: 
```{r, echo=FALSE, message=FALSE}
# population level slop estimate
pop_slope <- coef(model2u)[["Subject"]][,,'Days'][,1]
m2u_coefs <- coef(model2u)[["Subject"]][,,'Intercept'][,1]

estimate_df <- data.frame(subject = factor(names(m2u_coefs)),
           slope = pop_slope,
           intercept = m2u_coefs)


# Plotting first 4 subjects
# Function to emulate first 4 default colors from ggplot package
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors = gg_color_hue(4)

sleepstudy %>% 
    filter(Subject %in% c(308,309,310,330)) %>% # first 4 subjects
    ggplot(aes(x = Days, y = Reaction, color = Subject)) +
    geom_point() +
    # subject 308
    geom_abline(slope = estimate_df[1,]$slope, 
                intercept = estimate_df[1,]$intercept,
                color = colors[1]) +
    # subject 309
    geom_abline(slope = estimate_df[2,]$slope,
                intercept = estimate_df[2,]$intercept,
                color = colors[2]) +
    # subject 310
    geom_abline(slope = estimate_df[3,]$slope,
                intercept = estimate_df[3,]$intercept,
                color = colors[3]) +
    # subject 330
    geom_abline(slope = estimate_df[4,]$slope,
                intercept = estimate_df[4,]$intercept,
                color = colors[4]) +
  ggtitle("Sleep Study Model 2 - Uncorrelated")
```

***

## Model Comparison
There is an additional function `add_criterion()` that allows a model fit criteria to be added to a model object. There is no criteria added to the model build function by default. Instead, a separate call is needed after the model is built. 

Criteria choices include:

**loo**: Leave One Out cross-validation splits the data into training and testing sets, with the training set consisting of only one observation. During each iteration, the model is trained and a prediction is made on the left out observation.

**kfold**: K-fold cross validation splits data into k-groups, with one group designated as the test set. During each iteration, the model is trained on the k-1 training sets and predictions are made on the single test set.

**waic**: Widely Applicable Information Criterion is a generalization of the Akaike Information Criterion (AIC).

**loo_subsample**: An approximation of `loo` using subsampling for efficiency.

**bayes_r2**: Bayesian $r^2$ solves the problem with typical $r^2$ potentially having a numerator that is larger than the denominator.

**loo_r2**: Leave One Out $r^2$ is the $r^2$ adjusted for LOO cross-validation.

**marglik**: This criterion uses Log Marginal Likelihood for model comparison. This is also referred to as the model evidence and uses the Bayes factor to determine which model is best - $posterior\ odds = (prior\ odds * Bayes\ factor)$

Here, we will create a function that accepts a model object as an argument and will return the model with several criteria added.

```{r, message=FALSE, warning=FALSE}
# Function to add comparison criteria to a model
update_model <- function(model){
  model_crit <- add_criterion(model, c("waic", "loo", "bayes_R2"))
  model_crit$comparison <- data.frame(
      model_name = deparse(substitute(model)),
      waic = model_crit$criteria$waic$waic,
      loo = model_crit$criteria$loo$loo,
      bayes_R2 = mean(model_crit$criteria$bayes_R2)
    )
  return(model_crit)
}

# Add critera to each model
model1 <- update_model(model1)
model2c <- update_model(model2c)
model2u <- update_model(model2u)

# Build data frame with model criteria 
df <- rbind(model1$comparison, 
        model2c$comparison,
        model2u$comparison)

library(knitr)
knitr::kable(df)
```
For both WAIC and LOO we are looking for the model with the lowest number. In each case, WAIC is lower than LOO and Model 2 - Uncorrelated performs the best. 

For Bayes R2 we are looking for the highest number which would represent the most variance explained by the model. This criterion also chooses Model 2 - Uncorrelated as the best performing model.

Ultimately, you should check and compare several to make an informed decision on which model to proceed with in any given problem. 

***

## Nonlinear Mixed Models
We will utilize the `brms` package to approach a nonlinear problem with a Bayesian mixed model.

## The Dataset
The `lme4` package offers another dataset, "Contagious Bovine Pleuropneumonia" (CBPP) that is accessible using `data(cbpp)`. This dataset describes the serological incidence of CBPP in zebu cattle during a survey implemented in 15 commercial herds in Ethiopia. The data contains 4 colums:
1. `herd` - a factor identifying the herd
2. `incidence` - the number of new serological cases for a given herd and time period
3. `size` - the number describing the size of the herd at the beginning of a given time period
4. `period` - a factors with levels 1 to 4.

```{r}
head(cbpp)
```
We will treat this as a binomial problem with "successes" being the percentage of cattle from a given herd that did not have the disease at a given time period. First we'll plot the data points, colored by herd:

```{r}
cbpp %>% 
  ggplot(aes(x = period, y = (size-incidence)/size, group=herd)) + 
  geom_point(aes(col=herd)) +
  ylab("Cattle Without Disease (%)") + 
  xlab("Period")

```

It is a little difficult to see which points belong to which herd. We can add a line to make it a little more clear:

```{r}
cbpp %>% 
  ggplot(aes(x = period, y = (size-incidence)/size, group=herd)) + 
  geom_point(aes(col=herd)) +
  geom_line(aes(col=herd)) +
  ylab("Cattle Without Disease (%)") + 
  xlab("Period")

```
## Model 1 
The first model we will fit is a generalized linear mixed model with family "binomial". This model will have a random herd specific intercept and fixed 



$$logit(p_t)=\beta_0+\beta_{1,2}*I(period_t=2)+\beta_{1,3}*I(period_t=3)+\beta_{1,4}*I(period_t=4)+b_h$$ 
where 
* $\beta_0$ is the log odds of survival (not catching the disease) during period 1.
* $\beta_{1,t}$ is the difference in log odds between period 1 and period $t$
* $I(period_t= t)$ is an indicator variable that is 1 when the period is $t$ and 0 otherwise
* $b_h$ is the random herd level effect for herd $h$

The herd level random effects are assumed to come from an iid normal distribution with mean 0 and some shared, herd-level variance, $\sigma^2_h: b_h \sim N(0,\sigma^2_h)$

```{r, echo=TRUE, results = 'hide', cache=TRUE}
base_gm1 <- glmer(cbind(incidence, size - incidence) ~ period + (1 | herd), family = binomial, data = cbpp)
coef(base_gm1)
s <- summary(base_gm1)

# Variance components
sh <- s$varcor$herd[1,1]
se <- s$sigma # 1?

# Mean estimates
b0 <- s$coefficients[1,1]
b12 <- s$coefficients[2,1]
b13 <- s$coefficients[3,1]
b14 <- s$coefficients[4,1]

# SD of group intercepts
s1 <- sd(s$coefficients[,1])

priors <- c(prior(normal(-0.99, 10), class = b, coef = "period2"),
            prior(normal(-1.13, 10), class = b, coef = "period3"),
            prior(normal(-1.58, 10), class = b, coef = "period4"),
            prior(normal(0,0.41), class = Intercept),
            prior(normal(0, 1), class = sd),
            prior(normal(0, 0.26), class = sd, group = herd, coef = Intercept)
            )
            
gmm1 <- brm(incidence | trials(size) ~ period + (1 | herd), 
  # cbind(incidence, size - incidence) ~ period + (1 | herd), 
            family = binomial(), 
            prior = priors,
            chains = 4,
            iter = 10000,
            cores = 4,
            data = cbpp)

summary(gmm1)
```
Once the model is built we can access the herd level residual error, $\hat\sigma_h$ along with estimated error, credible intervals, $\hat R$, and ESS from the "Group-level Effects" section of the model summary. Below that, we have estimates for fixed population-level effects $\hat\sigma_0$, $\hat\sigma_{1,1}$, $\hat\sigma_{1,2}$, $\hat\sigma_{1,3}$, and $\hat\sigma_{1,4}$. 

We can also access the individual herd specific random intercepts using the `coef()` function:

```{r}
coef(gmm1)$herd[,,"Intercept"]
```


# Sources 

https://cran.microsoft.com/snapshot/2021-10-10/web/packages/lme4/vignettes/lmer.pdf  
https://www.lcampanelli.org/mixed-effects-modeling-lme4/  
https://www.rensvandeschoot.com/tutorials/lme4/  
https://vitalflux.com/fixed-vs-random-vs-mixed-effects-models-examples/  
https://www.rdocumentation.org/packages/lme4/versions/1.1-29/topics/lmer  
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5970551/  
https://towardsdatascience.com/a-bayesian-approach-to-linear-mixed-models-lmm-in-r-python-b2f1378c3ac8/  
https://towardsdatascience.com/evaluating-bayesian-mixed-models-in-r-python-27d344a03016/  
https://cran.r-project.org/web/packages/brms/index.html/  
https://yjunechoe.github.io/posts/2020-06-07-correlation-parameter-mem/  
https://towardsdatascience.com/statistics-101-credible-vs-confidence-interval-af7b7e8fdd79/  
https://mc-stan.org/  
https://avehtari.github.io/bayes_R2/bayes_R2.html   
https://aosmith.rbind.io/2020/08/20/simulate-binomial-glmm/ 

Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner (2021). Rank-normalization, folding, and localization: An improved R-hat for assessing convergence of MCMC (with discussion). Bayesian Data Analysis. 16(2), 667-–718. doi:10.1214/20-BA1221
(https://projecteuclid.org/journals/bayesian-analysis/advance-publication/Rank-Normalization-Folding-and-Localization--An-Improved-R%CB%86-for/10.1214/20-BA1221.full) 

Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003). 
