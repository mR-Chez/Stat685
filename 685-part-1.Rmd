---
title: "685 Part 1"
author: "Matt Chesney"
date: "6/5/2022"
output: html_document
---

```{r , echo=F, eval=F}
#data is built into package, this is an example for reference 
library(lme4)

# NOT RUN {
str(sleepstudy)
require(lattice)
xyplot(Reaction ~ Days | Subject, sleepstudy, type = c("g","p","r"),
       index = function(x,y) coef(lm(y ~ x))[1],
       xlab = "Days of sleep deprivation",
       ylab = "Average reaction time (ms)", aspect = "xy")
(fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy,REML=F))
## independent model
(fm2 <- lmer(Reaction ~ Days + (1|Subject) + (0+Days|Subject), sleepstudy, REML=F))
# }
(fm3 <- lmer(Reaction ~ Days + (Days||Subject), sleepstudy, REML=F))
```

## Introduction 

This is a tutorial to walk through of how to complete Mixed regression in R. The tutorial is in two parts, the first is mixed regression utilizing linear methods and part 2 is generalized linear regression. 


## Mixed models 
What are mixed models? introduce fixed and random effects and how they differ.

In an experiment there is a unit that the experiment is being conducted on, this is the **experimental unit**. For the Experimental unit there is some measured attribute of interest, this is the **independent variable** or our Y.  

A **fixed effect** is a treatment that is applied to our experimental unit. The treatment's effect is ascertained by a shift in the average value of Y, or the independent variable being monitored.

The NULL hypothesis is that the treatment has no effect on the mean. INSERT equations for null hypothesis and test statistic here. 

Examples:   
1. Comparing tensile strengths of alloy blends.  
2. Comparing weight gain in pigs from several food types  
3. Comparing maximum speeds of cars with different fuel additives
4. Comparing the blood pressure change in a patient over several days of medication 

A **random effect** is also applied to the experimental unit. The treatment's effect is determined by a difference in the variation in Y due to the application of the treatment. A random effect is assigned when the population of treatments is assumed to be homogeneous. 

The NULL hypothesis is that the treatment has no effect on the variance. INSERT equations for null hypothesis and test statistic here. 

Examples:  
1. A factory's various production lines effect on variation tensile strength.  
2. A pig farm's pig pens' effect on variation in weight gain  
3. A professional driver's effect on variation in top speeds of cars. 
4. A medical subject's effect on blood pressure medication treatment. 

The mixed model contains both fixed effects and random effects.There are several ways this relationship can manifest. The fixed effect can have a consistent effect within the random effect, intercept changes but slope remains the same. The fixed effect random effect can only vary in the presence of the fixed effect, intercept constant and slope varies. The fixed effect can vary with respect to the random effect, slope and intercept change. Each of these options will be examined in our sleep data example. 

Examples:  
1. Comparing tensile strengths of alloy blends from several production lines in a factory.  
2. Comparing weight gain in pigs from several food types living in one of several randomly assigned pens  
3. Comparing maximum speeds of cars with different fuel additives driven by randomly assigned professional drivers. 
4. Comparing the average blood pressure reduction after a treatment over several days applied to randomly selected subjects. 


## Introduction to the Data
The sleepstudy dataset is available as part of the lme4 package. It is from a study conducted by Belenkyn et all (2003). 

The response variable is the reaction time of the subjects to stimuli as recorded by researchers `Reaction`. The Subjects are the random effect, Subject. The number of days the subject underwent sleep deprivation is the fixed effect `Days`. 

```{r, echo=F}
library(lme4)
library(dplyr)
library(lattice)
library(ggplot2)
 
```
Below is the structure of the Sleep Study data 
```{r}
str(sleepstudy)
```


The first few lines of data. These are all observations for subject 308. 
```{r}

head(sleepstudy)
```

Plot of Sleep Study data, color coded by patient numbers  
```{r}


ggplot(sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject)) + 
    geom_point(size=1) + theme_classic()

```



## The Model
This example will follow the convention of using Greek symbols to denote fixed effects and capital letters to denote random effects. 

The variables in out model are: $y$ is the reaction time of our subjects, $\beta$ is the Days coefficient  
$C$, our random effect will be split into two parts: $C_{intercept}$ and $C_{slope}$. Our model for this example is below. 

Model 1, Only intercept varies 

$$y_{ij} = \alpha_{intercept}  + c_{i} + \beta_{j}*Days + e_{ij}$$



Model 2, intercept and slope vary and random terms are correlated  

$$y_{ij} = \alpha_{intercept}  + c_{i} + (\beta_{j}+d_{i})*Days + e_{ij}$$
Model 3, intercept and slope vary and random terms are not correlated  

$$y_{Reaction} = \alpha_{intercept}  + C_{intercept} + \beta*Days + \beta C_{slope}*Days + e_{residual}$$


## LME4 Package
To analyze the mixed effect model we will utilize the R package LME4. 

The relevant syntax for the model is as follows: 

Random intercept effect: `(1|RandomEffect)`

Random Slope effect: `(0+FixedEffect|RandomEffect)`

Random intercept and slope with random effect correlated:  
`1+FixedEffect+(1+FixedEffect|RandomEffect)`  
Abbreviation:  
`FixedEffect+(FixedEffect|RandomEffect`

Random intercept and slope with random effect uncorrelated:  
`1+FixedEffect+(1|RandomEffect)+(0+FixedEffect|RandomEffect)`  
Abbreviation:  
`FixedEffect+(FixedEffect||RandomEffect)`

We will utilize the abbreviated syntax where appropriate. 

### REML

Explain why REML is better to use than log likelihood 

## Model 1 

$$y_{Reaction} = \alpha_{intercept}  + C_{intercept} + \beta*Days + e_{residual}$$
```{r}
#add object to hold AIC, BIC, and LOG lik for four models 
Ac=0;Bc=0;ll=0 
```

For model comparison we will use the option ` REML=F`, once the model is selected we will use REML 

```{r, echo=F}
model1 <- lmer(Reaction ~ Days + (1|Subject), sleepstudy,REML=F)

summary(model1)
i=1
Ac[i]=AIC(model1)
Bc[i]=BIC(model1)
ll[i]=logLik(model1)

ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model1)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```

## Model 2 


$$y_{Reaction} = \alpha_{intercept} + \beta*Days + \beta C_{slope} + e_{residual}$$
```{r}
model2 <- lmer(Reaction ~ Days + (0+ Days|Subject), sleepstudy,REML=F)
summary(model2)
i=2
Ac[i]=AIC(model2)
Bc[i]=BIC(model2)
ll[i]=logLik(model2)

ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model2)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```

## Model 3 
$$y_{Reaction} = \alpha_{intercept}  + C_{intercept} + \beta*Days + \beta C_{slope}*Days + e_{residual}$$
### Correlated
```{r}
model3c <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy,REML=F)
summary(model3c)
i=3
Ac[i]=AIC(model3c)
Bc[i]=BIC(model3c)
ll[i]=logLik(model3c)

ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model3c)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()
```

### Uncorrelated
```{r}
model3u <- lmer(Reaction ~ Days + (Days||Subject), sleepstudy,REML=F)
summary(model3u)
i=4
Ac[i]=AIC(model3u)
Bc[i]=BIC(model3u)
ll[i]=logLik(model3u)


ggplot(NULL) + 
    geom_point(data=sleepstudy,aes(x=Days, y=Reaction, group=Subject, color=Subject))+geom_line(data=sleepstudy%>%mutate(pred_dist=fitted(model3u)),aes(x=Days, y=pred_dist, group=Subject, color=Subject))+ theme_classic()


```

```{r,echo=F}
#plot AIC, BIC, log lik of 4 models, explain which one will be used, 
name=c('model 1','model 2', 'model 3c','model 3u')
names(Ac)=name
names(Bc)=name
names(ll)=name

plot(Ac, ylab='AIC',type='h',lwd=20,xaxt='n'); axis(1,at=1:4,names(Ac))

plot(Bc, ylab='BIC',type='h',lwd=20,xaxt='n'); axis(1,at=1:4,names(Bc))

plot(ll, ylab='Log lik',type='h',lwd=20,xaxt='n'); axis(1,at=1:4,names(ll))

anova(model1,model2,model3c,model3u)



```
AIC and BIC select the uncorrelated model, log likelihood selects the correlated model. We will   

## Output 

## Explantion of output 

## bootstrap confidence intervals 

***

# Bayesian Methods
Bayesian methods are another option to approach mixed effects problems. Bayesian methods are especially helpful with smaller sample sizes, but are great for large samples as well. We will be using the *brms* package, which uses syntax similar to the *lme4* package. 

## Introduction to the dataset
We will be using a dataset that comes with the *lme4* pacakge called "sleepstudy". This dataset comes from a study conducted by Belenkyn et al (2003). 

In this dataset, we have 18 subjects with varying degrees of sleep deprivation.

```{r}
str(lme4::sleepstudy)
```

The response variable, `Reaction`, represents the average reaction time in milliseconds observed for subjects in a sleep deprivation study. `Days` represents the number of days the given `Subject` was deprived of sleep.

```{r}
head(lme4::sleepstudy)
```
Below we have a simple plot of the dataset.

```{r}
lme4::sleepstudy %>% 
    ggplot(aes(x = Days, y = Reaction)) + geom_point() + theme_classic()
```

Data plotted according to which Subject each measurement came from:

```{r}
lme4::sleepstudy %>% 
    ggplot(aes(x = Days, y = Reaction, color = Subject)) + geom_point() + theme_classic()
```
***

# Bayes Theorem
In Bayesian statistics the modelling approach is based off of Bayes' theorem, where we use information in the observed data to update knowledge about the model parameters. The background information is known as the prior distribution and combined with likelihood functions to determine the posterior distribution.

$$P(\theta, y)=P(\theta)P(y|\theta)$$
Where the posterior distribution is represented by: 
$$P(\theta, y)$$

The prior distribution:
$$P(\theta)$$
and the sampling distribution represented by:
$$P(y|\theta)$$
Given this information, we can use our data (sampling distribution) along with a prior to calculate the posterior.


## Model 1b  
Our first model assumes that the slope (reaction time over number of days) is the same for each subject. In this case, the goal is to estimate the intercept, which represents the baseline reaction time without sleep deprivation, for each subject. 

$$y_{ij} = \beta_0 + \beta_{1}x_j + C_{i} + e_{ij}$$
where yij is the reaction time of the $i^{th}$ subject at j days, $\beta_0$ is the population intercept, $\beta_1$ is the population slope, $x_j$ is the number of days at observation $j$, $C_i$ is the intercept for subject $i$, and $e_{ij}$ is the error term.

## Priors
We need to choose prior distributions for parameters that might be helpful in forming a posterior distribution. I will use parameters from our population data and fitted parameters from a linear model on the same data.

We need to first load in the *rstan* and *brms* packages which will allows us to build Bayesian models. 


```{r, message=FALSE}
require(rstan)
require(brms)

base_model <- lm(Reaction ~ Days, data=sleepstudy)

b0 <- round(base_model$coefficients[["(Intercept)"]])
b <- round(base_model$coefficients[["Days"]])
sig <- round(var(sleepstudy$Reaction))
sd <- round(sqrt(sig))
```

The priors we are using:
$$\beta_0 \sim N(251, 1)$$
$$\beta \sim N(10, 1)$$
$$\sigma \sim N(0, 3173)$$
$$\tau^2_0 \sim N(0, 56)$$

## BRMS package - Sampler
As mentioned before, this package does not fit models itself but uses `Stan` on the back end. Stan is a probabilistic programming language with a C++ backend. By default this backend uses the static Hamiltonian Monte-Carlo (HMC) Sampler and its extension the No-U-Turn Sampler (NUTS) by Hoffman and Gelman(2014). 

Alternatives to these algorithms, not available in the `brms` package include random-walk Metropolis and variations, Gibbs-sampling, and slice-sampling. The documentation for this package concludes that its choice of samplers converge much more quickly especially for high-dimensional models regardless of whether the priors are conjugate or not. Read the `brms` documentation overview for more information.

## The brm() function
This function in the *brms* package fits generalized linear/non-linear multivariate multilevel models (MMM). This function draws samples from the prior distribution, then combined with observed data calculates the posterior distribution.

To set priors, we use the *prior()* function, which accepts a distribution with parameters as well as a *class* argument which designates which parameter the prior is associated with. The `brm` function does not handle variables well, so you will need to manually enter the estimated values from the fitted model into the `prior()` function. 

## Default arguments for brm() function call
**Chains**: The number of Markov chains to use. You can select up to the number of cores your system has - defaults to 4.

**Iter**: The total number of iterations per chain. Some datasets may require more iterations to converge - meaning it takes longer to reach a point where further iterations do not improve the model. A higher number will cause the model to take longer to fit, see a comparison below. Defaults to 2,000.

**Warmup**: This refers to the number of iterations at the beginning of an MCMC run to be thrown away. The warm-up period is a hyper-parameter that is meant to give the Markov Chain time to reach its equilibrium distribution. The idea here is that a "bad" randomized starting point may over-sample regions that aren't representative of the equilibrium distribution. Defaults to `floor(iter/2)`.

**Thin**: Thinning consists of picking separated points from the sample, at each k-th step and removing them from the Markov chain. This helps with model convergence when there is correlation between data points and may speed up convergence on larger data sets. Defaults to 1.

**Family**: Defaults to Gaussian. Since we are building a *linear* mixed model, we will use leave use the default. More info on specifying distribution family can be found in the generalized mixed model section of this manual.

```{r, message=FALSE, warning=FALSE, echo=TRUE, cache=TRUE, results='hide'}
priors <- c(prior(normal(251, 1), class = Intercept), # intercept prior
            prior(normal(10, 1), class = b), # slope prior
            prior(normal(0, 3172), class = sigma), # population variance
            prior(normal(0, 56), class = sd) # tau0, group variance
            )

time_start <- Sys.time()
model1_2k <- brm(Reaction ~ Days + (1 | Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 2000,
             thin = 1, 
             family = gaussian()
             )
time_end <- Sys.time()
time_2k_iter <- time_end - time_start

time_start <- Sys.time()
model1 <- brm(Reaction ~ Days + (1 | Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 10000,
             # warmup = ,
             thin = 1, 
             family = gaussian()
             )
time_end <- Sys.time()
time_10k_iter <- time_end - time_start

summary(model1)
```

**Notes**: Output text is truncated. To compare these results with the output from a model built with the `lme4` package, please see the Frequentist section of this manual above. 

## Iteration Comparison
A comparison of fitting times:
**2k iterations**
```{r}
time_2k_iter
```

**10k iterations**
```{r}
time_10k_iter
```

More iterations took longer to fit, although in this case it is negligible. If a model fails to converge, you will see a warning output and may need to adjust iterations or other fitting options in order to be successful. Experiment with the arguments listed above until you reach an acceptable output. We will continue on using 10k iterations for this and the following models.

***

## Model Summary
With our model built, we can now get estimates of the parameters. R's built-in `summary()` function does a good job explaining each portion of the output one item at a time.

```{r}
summary(model1)
```

The first 5 lines of the summary output describe our model and model inputs defined in the `brm` call. The next section shows the Group-Level effects. Our sd estimate for intercept is 61.02, with Est.Error of 10.58 which represents the standard deviation of the distribution of the posterior.

### Group-Level Effects
We also have our lower and upper 95% credible interval for sd(intercept), (44.13, 84.86). Rhat is a measure of whether the samples obtained are likely representative of the true distribute. If this value is larger than 1, it is an indication that the model has not converged. Our Rhat is 1.00, meaning the model converged.

`Bulk_ESS` or bulk effective sample size is a diagnostic for the sampling efficiency in the bulk of the posterior distribution. It is defined as the effective sample size for rank normalized variable - in this case, the sd of the group intercept. The `Tail_ESS` represents the minimum of the effective sample sizes for 5% and 95% quantiles.

### Population-Level Effects
This section shows our intercept estimate, $\alpha$ = 204.98, our slope estimate, $\beta_{j}$ = 10.28 along with credible intervals, Rhat, and bulk/tail ESS for each.

### Family Specific Parameters:
This shows our estimate for $\sigma$ which accounts for residual error variance. 

***

## Model Output - Posterior Summary
While the `summary()` function has a tidy appearance, the `posterior_summary()` function produces much more information about the estimates. 
```{r}
posterior_summary(model1)
```

The first 4 lines are repeated from the summary, along with estimate quantiles. You can also view a plot of these estimates and visually compare the posterior distributions of each to a normal distribution:
```{r}
plot(model1)
```

The next section of our model output displays $C_{i}$, our intercept estimates for each subject. The means are interpreted as where the intercept is on the Y-axis in relation to the population mean. For example - subject 308 has an intercept mean of 88.46, so this could be interpreted as the subject having a reaction time of (88.46 + 204.98) = 293.44ms. 

`lprior` and `lp__` are used for automatic prior/likelihood sensitivity analysis using an additional package, `priorsens` and are not included in the `brms` documentation.

## Plotting Model Estimates
We can pull our model coefficients and error estimates using the `coef()` function. 
```{r}
# population level slop estimate
pop_slope <- coef(model1)[["Subject"]][,,'Days'][1,1]
m1_coefs <- coef(model1)[["Subject"]][,,'Intercept'][,1]

estimate_df <- data.frame(subject = factor(names(m1_coefs)),
           slope = pop_slope,
           intercept = m1_coefs)


# Plotting first 4 subjects
# Function to emulate first 4 default colors from ggplot package
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors = gg_color_hue(4)

sleepstudy %>% 
    filter(Subject %in% c(308,309,310,330)) %>% # first 4 subjects
    ggplot(aes(x = Days, y = Reaction, color = Subject)) +
    geom_point() +
    # subject 308
    geom_abline(slope = estimate_df[1,]$slope, 
                intercept = estimate_df[1,]$intercept,
                color = colors[1]) +
    # subject 309
    geom_abline(slope = estimate_df[2,]$slope,
                intercept = estimate_df[2,]$intercept,
                color = colors[2]) +
    # subject 310
    geom_abline(slope = estimate_df[3,]$slope,
                intercept = estimate_df[3,]$intercept,
                color = colors[3]) +
    # subject 330
    geom_abline(slope = estimate_df[4,]$slope,
                intercept = estimate_df[4,]$intercept,
                color = colors[4])
```

## Model 2
Our second model has a random intercept term and a random slope, with correlated random effect. 
$$y_{ij} = \beta_0  + \beta_1 x_j + C_{i,0}  + C_{i,1}*x_j + e_{ij}$$
Reaction time $y_{ij}$ is equal to our population slope $\beta_0$ plus population intercept $\beta_1 x$ times days at observation $j$, plus subject level intercept $C_{i,0}$ plus subject level slope $C_{i,1}$ times number of days at observation $j$, plus our error term $e_{ij}$. 

We will first build this model with random effects `Days` and `Subject` correlated.

## Model 2 - Correlated 
```{r, message=FALSE, warning=FALSE, echo=TRUE, cache=TRUE, results='hide'}
model2c <- brm(Reaction ~ Days + (Days | Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 10000,
             thin = 1, 
             family = gaussian()
             )

# summary(model2c)
posterior_summary(model2c)
```
Now we can see all estimates for slopes and intercepts in each group and population-wide. The first two lines, `b_Intercept` and `b_Days` represent values for the population parameters $\beta_0$ and $\beta_1$, respectively. The next two lines represent the standard deviation of the Subject-level posterior distributions.

The next line, `cor_Subject__Intercept__Days` is the estimated correlation between Subjects and Days - which is 0.213 or a very weak to weak positive association. This suggests that per Subject, the intercept is weakly associated with the independent variable Days. Sigma represents the overall variance of predicted values.

The next 36 lines show our model estimates for intercept and days for each individual subject, along with posterior standard deviation (Est.Error), 2.5%, and 97.5% credible intervals.

```{r}
coef(model2c)
posterior_summary(model2c)
```

```{r, echo=TRUE, message=FALSE}
# population level slop estimate
pop_slope <- coef(model2c)[["Subject"]][,,'Days'][,1]
m2c_coefs <- coef(model2c)[["Subject"]][,,'Intercept'][,1]

estimate_df <- data.frame(subject = factor(names(m2c_coefs)),
           slope = pop_slope,
           intercept = m2c_coefs)


# Plotting first 4 subjects
# Function to emulate first 4 default colors from ggplot package
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors = gg_color_hue(4)

sleepstudy %>% 
    filter(Subject %in% c(308,309,310,330)) %>% # first 4 subjects
    ggplot(aes(x = Days, y = Reaction, color = Subject)) +
    geom_point() +
    # subject 308
    geom_abline(slope = estimate_df[1,]$slope, 
                intercept = estimate_df[1,]$intercept,
                color = colors[1]) +
    # subject 309
    geom_abline(slope = estimate_df[2,]$slope,
                intercept = estimate_df[2,]$intercept,
                color = colors[2]) +
    # subject 310
    geom_abline(slope = estimate_df[3,]$slope,
                intercept = estimate_df[3,]$intercept,
                color = colors[3]) +
    # subject 330
    geom_abline(slope = estimate_df[4,]$slope,
                intercept = estimate_df[4,]$intercept,
                color = colors[4]) +
  ggtitle("Sleep Study Model 2 - Correlated")
```

## Model 2 - Uncorrelated
Now we build the model with random effects `Days` and `Subject` uncorrelated. This assumes that the random effects between the two are uncorrelated, and so the output will not show the `cor` estimate that we saw above.

```{r, message=FALSE, echo=TRUE, cache=TRUE, results='hide'}
model2u <- brm(Reaction ~ Days + (Days || Subject),
             data = sleepstudy,
             prior = priors,
             chains = 4,
             iter = 10000,
             thin = 1, 
             family = gaussian()
             )

posterior_summary(model2u)
```

As in previous model outputs, the columns here show the estimated parameter name, mean estimate, posterior standard deviation, 2.5% and 97.5% credible intervals. 
```{r}
posterior_summary(model2u)
```

```{r, echo=TRUE, message=FALSE}
# population level slop estimate
pop_slope <- coef(model2u)[["Subject"]][,,'Days'][,1]
m2u_coefs <- coef(model2u)[["Subject"]][,,'Intercept'][,1]

estimate_df <- data.frame(subject = factor(names(m2u_coefs)),
           slope = pop_slope,
           intercept = m2u_coefs)


# Plotting first 4 subjects
# Function to emulate first 4 default colors from ggplot package
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

colors = gg_color_hue(4)

sleepstudy %>% 
    filter(Subject %in% c(308,309,310,330)) %>% # first 4 subjects
    ggplot(aes(x = Days, y = Reaction, color = Subject)) +
    geom_point() +
    # subject 308
    geom_abline(slope = estimate_df[1,]$slope, 
                intercept = estimate_df[1,]$intercept,
                color = colors[1]) +
    # subject 309
    geom_abline(slope = estimate_df[2,]$slope,
                intercept = estimate_df[2,]$intercept,
                color = colors[2]) +
    # subject 310
    geom_abline(slope = estimate_df[3,]$slope,
                intercept = estimate_df[3,]$intercept,
                color = colors[3]) +
    # subject 330
    geom_abline(slope = estimate_df[4,]$slope,
                intercept = estimate_df[4,]$intercept,
                color = colors[4]) +
  ggtitle("Sleep Study Model 2 - Uncorrelated")
```

***

## Model Comparison
There is an additional function `add_criterion()` that allows a model fit criteria to be added to a model object. There is no criteria added to the model build function by default. Instead, a separate call is needed after the model is built. 

Criteria choices include:

**loo**: Leave One Out cross-validation splits the data into training and testing sets, with the training set consisting of only one observation. During each iteration, the model is trained and a prediction is made on the left out observation.

**kfold**: K-fold cross validation splits data into k-groups, with one group designated as the test set. During each iteration, the model is trained on the k-1 training sets and predictions are made on the single test set.

**waic**: Widely Applicable Information Criterion is a generalization of the Akaike Information Criterion (AIC).

**loo_subsample**: An approximation of `loo` using subsampling for efficiency.

**bayes_r2**: Bayesian $r^2$ solves the problem with typical $r^2$ potentially having a numerator that is larger than the denominator.

**loo_r2**: Leave One Out $r^2$ is the $r^2$ adjusted for LOO cross-validation.

**marglik**: This criterion uses Log Marginal Likelihood for model comparison. This is also referred to as the model evidence and uses the Bayes factor to determine which model is best - $posterior\ odds = (prior\ odds * Bayes\ factor)$

```{r, message=FALSE, warning=FALSE}
# Function to add comparison criteria to a model
update_model <- function(model){
  model_crit <- add_criterion(model, c("waic", "loo", "bayes_R2"))
  model_crit$comparison <- data.frame(
      model_name = deparse(substitute(model)),
      waic = model_crit$criteria$waic$waic,
      loo = model_crit$criteria$loo$loo,
      bayes_R2 = mean(model_crit$criteria$bayes_R2)
    )
  return(model_crit)
}

# Add critera to each model
model1 <- update_model(model1)
model2c <- update_model(model2c)
model2u <- update_model(model2u)

# Build data frame with model criteria 
df <- rbind(model1$comparison, 
        model2c$comparison,
        model2u$comparison)

df
```
For both WAIC and LOO we are looking for the model with the lowest number. In each case, WAIC is lower than LOO and Model 2 - Uncorrelated performs the best. 

For Bayes R2 we are looking for the highest number which would represent the most variance explained by the model. This criterion also chooses Model 2 - Uncorrelated as the best performing model.

Ultimately, you should check and compare several to make an informed decision on which model to proceed with in any given problem. 

# Sources 

https://cran.microsoft.com/snapshot/2021-10-10/web/packages/lme4/vignettes/lmer.pdf  
https://www.lcampanelli.org/mixed-effects-modeling-lme4/  
https://www.rensvandeschoot.com/tutorials/lme4/  
https://vitalflux.com/fixed-vs-random-vs-mixed-effects-models-examples/  
https://www.rdocumentation.org/packages/lme4/versions/1.1-29/topics/lmer  
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5970551/  
https://towardsdatascience.com/a-bayesian-approach-to-linear-mixed-models-lmm-in-r-python-b2f1378c3ac8
https://towardsdatascience.com/evaluating-bayesian-mixed-models-in-r-python-27d344a03016
https://cran.r-project.org/web/packages/brms/index.html
https://yjunechoe.github.io/posts/2020-06-07-correlation-parameter-mem/
https://towardsdatascience.com/statistics-101-credible-vs-confidence-interval-af7b7e8fdd79/
https://mc-stan.org/ 
https://avehtari.github.io/bayes_R2/bayes_R2.html 

Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003). 
